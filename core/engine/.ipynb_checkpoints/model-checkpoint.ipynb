{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using keras to create a text classifier\n",
    "#Highly inspired from https://github.com/iampukar/toxic-comments-classification\n",
    "\n",
    "#importing libraries\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import RNN, GRU, LSTM, Dense, Input, Embedding, Dropout, Activation, concatenate\n",
    "from keras.layers import Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "#Loading the dataset\n",
    "train_data = pd.read_csv('AI/train.csv')\n",
    "\n",
    "#Extracting the labels\n",
    "classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_data[classes].values\n",
    "\n",
    "#Lowercase\n",
    "train_sentences = train_data[\"comment_text\"].fillna(\"fillna\").str.lower()\n",
    "\n",
    "max_features = 100000\n",
    "max_len = 150\n",
    "embed_size = 300\n",
    "\n",
    "#Tokenize (creates a word index, converting the most recurrent words into floats)\n",
    "tokenizer = Tokenizer(max_features)\n",
    "tokenizer.fit_on_texts(list(train_sentences))\n",
    "\n",
    "#Convert the sentences of the dataset into lists of floats using the tokenizer\n",
    "tokenized_train_sentences = tokenizer.texts_to_sequences(train_sentences)\n",
    "\n",
    "#Pad the lists with zeroes so that every list has the same size to be fed to the neural network\n",
    "train_padding = pad_sequences(tokenized_train_sentences, max_len)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "\n",
    "#Complex neural network\n",
    "image_input = Input(shape=(max_len, ))\n",
    "X = Embedding(max_features, embed_size)(image_input)\n",
    "X = Bidirectional(GRU(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(X)\n",
    "avg_pl = GlobalAveragePooling1D()(X)\n",
    "max_pl = GlobalMaxPooling1D()(X)\n",
    "conc = concatenate([avg_pl, max_pl])\n",
    "X = Dense(6, activation=\"sigmoid\")(conc)\n",
    "model = Model(inputs=image_input, outputs=X)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "saved_model = \"cp.ckpt\"\n",
    "cp_callback = ModelCheckpoint(saved_model, save_weights_only=True, verbose=1, save_best_only=True)\n",
    "\n",
    "#Training\n",
    "batch_sz = 32\n",
    "epoch = 2\n",
    "model.fit(train_padding, y, batch_size=batch_sz, epochs=epoch, validation_split=0.1, callbacks=[cp_callback], steps_per_epoch = 1024)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
